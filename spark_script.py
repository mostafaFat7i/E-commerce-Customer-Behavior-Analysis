# -*- coding: utf-8 -*-
"""Assignment1-bigdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1plOpNw6BF4U--vX1AvVFlXOxLJTeT-DT
"""

#Steps in Spark container

from pyspark.sql.types import StructType, StructField, StringType

data_path = "hdfs://namenode:9000/assignment1/output01/part-r-00000"
data_rdd = spark.sparkContext.textFile(data_path)
# Extract header
header = data_rdd.take(1)[0]
data_rows = data_rdd.filter(lambda line: line != header)
formatted_data = data_rows.map(lambda x: x.replace('\t', ','))
split_data = formatted_data.map(lambda line: line.split(","))
schema = StructType([
        StructField("Customer ID", StringType(), True),
        StructField("Gender", StringType(), True),
        StructField("Age", StringType(), True),
        StructField("City", StringType(), True),
        StructField("Total Spend", StringType(), True),
        StructField("Items Purchased", StringType(), True),
        StructField("Satisfaction Level", StringType(), True)
    ])
df = spark.createDataFrame(split_data , schema=schema)
output_path = "hdfs://namenode:9000/assignment1/output_csv"
csv_data = split_data.map(lambda line: ",".join(line))
csv_data.saveAsTextFile(output_path)

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

!ls

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql.types import StructType, StructField, StringType
schema = StructType([
        StructField("Customer ID", StringType(), True),
        StructField("Gender", StringType(), True),
        StructField("Age", StringType(), True),
        StructField("City", StringType(), True),
        StructField("Total Spend", StringType(), True),
        StructField("Items Purchased", StringType(), True),
        StructField("Satisfaction Level", StringType(), True)
    ])
df = spark.read.option("header", "true").schema(schema).csv('/content/drive/MyDrive/output_csv')
df.show()

df.count()

df.printSchema()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.sql.functions import col

import matplotlib.pyplot as plt
import pandas as pd

"""Can we segment customers based on their demographic information (Age,Gender, City) and shopping behaviors (Total Spend, Number of Items Purchased,
Membership Type)?
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("CustomerAnalysis").getOrCreate()

# Define schema
schema = StructType([
    StructField("Customer ID", StringType(), True),
    StructField("Gender", StringType(), True),
    StructField("Age", DoubleType(), True),
    StructField("City", StringType(), True),
    StructField("Total Spend", DoubleType(), True),
    StructField("Items Purchased", DoubleType(), True),
    StructField("Satisfaction Level", StringType(), True)
])

# Read data with schema
df = spark.read.option("header", "true").schema(schema).csv('/content/drive/MyDrive/output_csv')
df = df.filter(~col("Customer ID").startswith("Customer ID"))

# Check and handle missing values
df = df.na.drop(subset=["Age", "Total Spend", "Items Purchased"])

# Index categorical columns
gender_indexer = StringIndexer(inputCol="Gender", outputCol="GenderIndex")
city_indexer = StringIndexer(inputCol="City", outputCol="CityIndex")
satisfaction_indexer = StringIndexer(inputCol="Satisfaction Level", outputCol="SatisfactionIndex")

# Assemble features
assembler = VectorAssembler(
    inputCols=["Age", "GenderIndex", "CityIndex", "Total Spend", "Items Purchased", "SatisfactionIndex"],
    outputCol="features"
)

# 1. Segment customers based on demographic information and shopping behaviors
kmeans = KMeans(k=3, featuresCol="features", predictionCol="segment")
pipeline = Pipeline(stages=[gender_indexer, city_indexer, satisfaction_indexer, assembler, kmeans])
model = pipeline.fit(df)
segmented_df = model.transform(df)

# Save segmentation results
segmented_df.select("Customer ID", "segment").write.mode("overwrite").csv("charts/customer_segments")
segmented_df.show()

import matplotlib.pyplot as plt

# Create and save the scatter plot
segments_plot = segmented_df.toPandas()

# Customize the plot based on your specific requirements
plt.scatter(segments_plot['Total Spend'], segments_plot['Items Purchased'], c=segments_plot['segment'], cmap='viridis')
plt.xlabel('Total Spend')
plt.ylabel('Items Purchased')
plt.title('Customer Segmentation')
plt.colorbar(label='Segment')

# Save the plot
plt.savefig("charts/customer_segments_plot.png")

# Show the plot
plt.show()



"""Which customers are at risk of not making future purchases based on their Days?"""

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, round
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("CustomerAnalysis").getOrCreate()

# Define schema
schema = StructType([
    StructField("Customer ID", StringType(), True),
    StructField("Gender", StringType(), True),
    StructField("Age", StringType(), True),
    StructField("City", StringType(), True),
    StructField("Total Spend", StringType(), True),
    StructField("Items Purchased", StringType(), True),
    StructField("Satisfaction Level", StringType(), True)
])

# Read data with schema
df = spark.read.option("header", "true").schema(schema).csv('/content/drive/MyDrive/output_csv')
df = df.filter(~col("Customer ID").startswith("Customer ID"))


# Cast relevant columns to double
df = df.withColumn("Age", col("Age").cast(DoubleType()))
df = df.withColumn("Total Spend", col("Total Spend").cast(DoubleType()))
df = df.withColumn("Items Purchased", col("Items Purchased").cast(DoubleType()))

# Check and handle missing values
df = df.na.drop(subset=["Age", "Total Spend", "Items Purchased"])

# Select only the columns that have been cast to double
selected_cols = ["Customer ID", "Age", "Total Spend", "Items Purchased", "Satisfaction Level"]
df_selected = df.select(selected_cols)

# Assume a proxy for "Days Since Last Purchase" based on the inverse of "Items Purchased"
df_selected = df_selected.withColumn("Days Since Last Purchase", 1 / col("Items Purchased"))

# Round the "Days Since Last Purchase" to 3 decimal places
df_selected = df_selected.withColumn("Days Since Last Purchase", round(col("Days Since Last Purchase"), 3))

# Check and handle non-numeric values
for col_name in selected_cols:
    df_selected = df_selected.filter(col(col_name).isNotNull())

# Convert the "Satisfaction Level" column to a numeric label
indexer = StringIndexer(inputCol="Satisfaction Level", outputCol="label")
df_selected_indexed = indexer.fit(df_selected).transform(df_selected)

# 2. Identify customers at risk of not making future purchases
assembler_risk = VectorAssembler(
    inputCols=["Days Since Last Purchase", "label"],
    outputCol="risk_features"
)
rf_classifier = RandomForestClassifier(labelCol="label", featuresCol="risk_features")
pipeline_risk = Pipeline(stages=[assembler_risk, rf_classifier])
model_risk = pipeline_risk.fit(df_selected_indexed)

# Predict the risk and save results
risk_df = model_risk.transform(df_selected_indexed)

risk_df.show()

# Filter for customers at risk (Unsatisfied)
customers_at_risk = risk_df.filter(col("prediction") == 1.0)

# Select and display Customer IDs
customer_ids_at_risk = customers_at_risk.select("Customer ID").collect()
for row in customer_ids_at_risk:
    print(row["Customer ID"])

import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Assuming you have a SparkSession named 'spark'
# If not, create one
spark = SparkSession.builder.appName("CustomerRiskAnalysis").getOrCreate()

# Your existing code for filtering customers at risk
customers_at_risk = risk_df.filter(col("prediction") == 1.0)

# Create a DataFrame with all customer IDs
all_customer_ids = risk_df.select("Customer ID").distinct()

# Identify customers not at risk
customers_not_at_risk = all_customer_ids.subtract(customers_at_risk.select("Customer ID"))

# Convert Spark DataFrames to Pandas DataFrames for local processing
customers_at_risk_pd = customers_at_risk.toPandas()
customers_not_at_risk_pd = customers_not_at_risk.toPandas()

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(["At Risk", "Not at Risk"], [len(customers_at_risk_pd), len(customers_not_at_risk_pd)], color=['red', 'green'])
plt.title("Customers at Risk vs Not at Risk")
plt.xlabel("Risk Level")
plt.ylabel("Number of Customers")
plt.savefig("charts/Customers at Risk vs Not at Risk.png")

plt.show()

"""Can we predict a customerâ€™s Satisfaction Level based on their demographic and purchase history data?"""

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.appName("CustomerSatisfactionPrediction").getOrCreate()

# Define schema
schema = StructType([
    StructField("Customer ID", StringType(), True),
    StructField("Gender", StringType(), True),
    StructField("Age", IntegerType(), True),
    StructField("City", StringType(), True),
    StructField("Total Spend", DoubleType(), True),
    StructField("Items Purchased", IntegerType(), True),
    StructField("Satisfaction Level", StringType(), True)
])

# Read the data with the corrected schema
data_path = "/content/drive/MyDrive/output_csv"
df = spark.read.csv(data_path, header=True, schema=schema)
df = df.filter(~col("Customer ID").startswith("Customer ID"))



# Handle missing values in the "Age" column
df = df.na.fill({'Age': 0})  # You might want to choose a different strategy based on your data

# Feature engineering
gender_indexer = StringIndexer(inputCol="Gender", outputCol="GenderIndex")
city_indexer = StringIndexer(inputCol="City", outputCol="CityIndex")

# Assemble features with handling null values
feature_columns = ["Age", "Total Spend", "Items Purchased", "GenderIndex", "CityIndex"]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features", handleInvalid="keep")

# Encode target variable
label_indexer = StringIndexer(inputCol="Satisfaction Level", outputCol="label")

# Define the model
dt_classifier = DecisionTreeClassifier(featuresCol="features", labelCol="label")

# Create a pipeline
pipeline = Pipeline(stages=[gender_indexer, city_indexer, assembler, label_indexer, dt_classifier])

# Split the data into training and testing sets
(training_data, testing_data) = df.randomSplit([0.8, 0.2], seed=42)

try:
    # Train the model
    model = pipeline.fit(training_data)

    # Make predictions on the testing set
    predictions = model.transform(testing_data)

    # Evaluate the model
    evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    print("Accuracy: {:.2%}".format(accuracy))

except Exception as e:
    import traceback
    traceback.print_exc()

from pyspark.ml.feature import StringIndexerModel

# Get the fitted StringIndexerModel from the pipeline
indexer_model = model.stages[3]  # Assuming StringIndexer is the 4th stage in your pipeline

# Get the original labels used for indexing
original_labels = indexer_model.labels

# Display the mapping
label_mapping = dict(enumerate(original_labels))
print("Label Mapping:")
for label, original_label in label_mapping.items():
    print(f"Numeric Label {label} corresponds to '{original_label}'")

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import PipelineModel

# Create a Spark session
spark = SparkSession.builder.appName("CustomerSatisfactionPrediction").getOrCreate()

# Define schema
schema = StructType([
    StructField("Customer ID", StringType(), True),
    StructField("Gender", StringType(), True),
    StructField("Age", IntegerType(), True),
    StructField("City", StringType(), True),
    StructField("Total Spend", DoubleType(), True),
    StructField("Items Purchased", IntegerType(), True),
    StructField("Satisfaction Level", StringType(), True)
])

# Generate three samples for prediction
new_data_samples = [
    (300, "Male", 33, "Chicago", 800.5, 14),
    (301, "Female", 29, "New York", 1200.7, 17),
    (302, "Male", 35, "Los Angeles", 700.8, 11)
]

# Create a new schema without the "Satisfaction Level" field
new_schema = StructType(schema.fields[:-1])

# Create a DataFrame for the new data
new_data_df = spark.createDataFrame(new_data_samples, schema=new_schema)

# Handle missing values in the "Age" column
new_data_df = new_data_df.na.fill({'Age': 0})  # You might want to choose a different strategy based on your data

# Transform the new data using the pipeline
new_predictions = model.transform(new_data_df)

# Select relevant columns for display
result = new_predictions.select("Customer ID", "features", "prediction")

# Show the result
result.show()













